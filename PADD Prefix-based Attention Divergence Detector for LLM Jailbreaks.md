# Abstract
大型语言模型（大语言模型）在理解和生成任务方面取得了显著进展，但它们仍然非常容易受到绕过安全保障措施并诱导生成有害内容的对抗性即时攻击。现有的前代和后代防御方法通常依赖于意图识别、表面级特征或外部分类器，这使得它们容易通过微妙的提示扰动进行规避，同时产生大量的计算开销。
在本文中，我们提出了PADD（基于前缀的注意力分歧检测器），这是一种轻量级的预生成防御机制，利用内部模型信号进行鲁棒攻击检测。在其核心，PADD在输入提示前加上一个轻量级安全前缀，并比较原始提示和前缀提示之间的注意力分布。通过通过注意力发散和注意力可塑性的复合信号将交叉提示比较转换为自我比较，PADD通过低延迟检测实现了对抗性和良性提示之间的强可分离性，而不需要修改或微调基本模型。跨四个主流开源大语言模型和多个公共基准的广泛实验表明，PADD显著降低了攻击成功率（0.4-3.0%），同时保持了较低的错误拒绝率（0-5.2%）。这些结果将PADD定位为大语言模型安全的可扩展、高效和实用的保障措施。
# 1 Introduction
## 1.1 Motivation
由于大型语言模型（大语言模型）的高级语言理解能力，许多web服务提供商已经在其应用程序中集成了大语言模型。然而，这不可避免地引入了安全漏洞：对抗性提示可以绕过安全措施，引发不安全或违反策略的输出【39, 51】，这种危害在web环境中被放大。最近的研究表明，这种攻击在现实世界中基于大语言模型的web应用程序中无处不在并不断发展，降低了可用性和安全性[36, 42]。现有的后生成防御在完成后验证输出【28, 33】，但它们不足以及时进行用户交互。因此，迫切需要在产生任何响应之前主动有效地检测对抗性提示，从而在交互的最早阶段降低风险。
在现实世界的大语言模型部署中，防御时机至关重要。一旦后生成方法检测到越狱、不安全的内容（即攻击性语言、歧视性言论、错误信息，甚至是非法活动的指示）已经产生，导致往往不可逆转的伤害【10, 11, 39】。此外，由于大语言模型应用程序的流性质，此类内容可能已经显示给用户、被系统记录或传播到下游组件【12, 26】。这些风险强调了生成前防御的必要性：在生成任何文本之前主动识别和拦截高风险提示，从而在交互的最早阶段防止伤害。
一条研究线设计了评估大语言模型提示安全性的预生成检测器，如基于嵌入的分类器[2]、令牌级统计异常检测器[15]和外部训练的提示分类器[16]。然而，这些方法容易受到同义词替换、微小扰动、表面混淆和其他将对抗性提示伪装成良性提示的技术的影响。它们还严重依赖于表层特征（例如，提示模板和格式），并需要大型标记数据集，导致高维护成本和较差的跨模型或跨领域泛化[4, 17，31]。另一个研究方向提出了评估生成响应安全性的后生成防御，包括危害性分类器[28]和安全前缀缓解[47]。然而，如前所述，这些方法不能防止不可逆转的下游损害[40]。
图1给出了三个具体的例子。第一个提示虽然是良性的，但包含敏感词“kill”，这很容易在基于嵌入和令牌级别的解决方案中触发错误拒绝。第二个提示是由GCG[51]有针对性地生成的后缀的对抗性提示，因此成功地绕过了大多数现有的检测解决方案。第三个提示是另一个具有越狱风格模板和格式的对抗性提示。随着对抗性提示的不断演变，需要先验知识（例如，需要训练或依赖于表面级模式）的分类器很难识别它们。
【图1：检测对抗性提示的示例】
这些证明了在响应生成之前检测对抗性提示的三个主要挑战。
#### 1）对抗性提示和良性提示之间的相似性
对抗性提示通常被故意制作成类似于良性提示，使得基于关键字的规则、表层启发法和基于嵌入的检测器在区分有害和安全输入方面无效。如图1所示，对抗性和良性提示都可能包括关键字kill，从而导致高度相似的表面和嵌入特征。这一挑战需要一种鲁棒的解决方案，即使在高表面或语义相似性下也能提取有区别的信号。
#### 2）提示模板和格式的可变性
大语言模型以自由文本提示作为输入，具有多样化的模板和格式。这种多样性对标记化过程和嵌入表示具有实质性影响，导致在有限的提示数据集上训练的检测器的分布外问题。为不同的现实世界提示设计一个通用的解决方案仍然是一个悬而未决的问题。
#### 3）缺乏明确的越狱信号
对抗性提示探测器依靠越狱信号来指导决策。理想情况下，这种信号应该是低维的，并且可以用少量样本进行校准。然而，当前的实现通常不能满足这些要求，导致高计算成本并且缺乏对数字信号的稳定的、可阈值的解释。为了实现实时预生成防御，我们必须识别越狱信号，以可靠地区分高风险和良性提示，支持低延迟部署，并允许使用最少的标记数据进行稳定校准——这是一项具有挑战性的任务。
## 1.2 Contribution
在本文中，我们提出了PADD，一种在预生成阶段检测对抗性提示和大语言模型越狱的自动化工具。
为了解决挑战-1和2，PADD采用了基于探针的策略，在原始提示前加上一个短的安全前缀，放大了对抗性和良性输入之间的区别。然后，它将原始提示和前缀提示输入大语言模型，并比较它们的隐藏状态（即注意力张量），这些状态捕捉令牌焦点和高级语义信息。我们观察到，在安全前缀下，对抗性提示通常会引起注意力分布的显著和不稳定的变化，而良性提示保持稳定并灵活适应。利用这一属性，PADD捕获提示意图的差异，而不依赖于表面级模式。特别是，它将位置感知规范化和压缩应用于注意力张量，产生低维特征摘要。
为了应对挑战3，PADD设计了一个风险评分函数，该函数融合了特征摘要，以表征原始提示和前缀提示之间的差异，因为更大的差异对应于更高的越狱风险。为了确定阈值，我们设计了一个轻量级校准管道，该管道利用统计方法和每个大语言模型的小型验证集。
我们评估了多个大语言模型家族和四个公共对抗性提示基准的PADD。评估结果显示，PADD将所有模型的平均攻击成功率（ASR）降低至0.4-3.0%，比基线低70-90%。PADD还保持了0-5.2%的低错误拒绝率（FRR），显著优于基线。
# 2 Preliminaries and Main Idea
## 2.1 Problem Formulation
本文旨在在响应生成之前检测用户发出的提示是否是对抗性提示。
输入-输入是目标大语言模型M和用户发出的提示x。M是不可修改的，所有参数都是可访问的。
对抗性提示的定义-从广义上讲，任何带有恶意意图、试图诱导模型产生不安全内容的提示都可以被视为对抗性提示。在本文中，只有当提示实际上能够绕过特定模型的安全护栏并导致有害输出时，我们才将其视为对抗性的[20]。
任务目标-我们的目标是在生成任何响应之前确定给定的提示x是否是对抗性的。形式上，我们定义了一个风险评分函数J（x），它估计x是对抗性的可能性。根据阈值τ将提示分类为对抗性或良性：
Judge(x)=I{J(x)>τ}，
其中I{·}表示指示符函数，如果条件成立，则返回1，否则返回0。因此，J（x）>τ的提示被标记为对抗性的，而其余的被视为良性的。该公式将连续分数转换为可部署的二元决策。注意，阈值τ是模型相关的，并且其最佳值可能在不同的大语言模型上变化。
![[Pasted image 20251208195424.png]]
## 2.2 Main Ideas
我们展示了解决方案背后的关键思想。
#### 意见1
当添加轻量级安全前缀时，对抗性和良性提示在其注意力分布中显示出根本不同的行为。具体来说，对抗性提示通常会引起显著和不稳定的注意力转移，而良性提示会保持稳定或平稳地适应干扰。这表明，即使这两种类型的提示在表面水平上看起来相似，它们也会在模型中引发系统不同的内部响应。
#### 关键思想1：安全前缀作为探针
受这一观察结果的启发，PADD采用固定的安全前缀作为受控探针。通过比较原始提示和前缀提示的内部响应，PADD放大了意图相关的差异，并获得了独立于表面模式的可靠、自比较的信号。
#### 意见2
对抗性提示和良性提示之间的差距不仅明显，而且在信号水平上是稳定的。具体来说，对抗性提示表现出对局部扰动敏感的注意模式，放大不相关的噪声并产生不稳定的波动。相反，良性提示产生一致的注意力分布，在不同条件下保持结构稳定性。这种相对稳定性为区分对抗性提示和良性提示提供了可靠的基础。
#### 关键思想2：将稳定的差异压缩成判别信号
基于这一观察，我们注意到对抗性和良性提示在注意力反应的稳健性上始终不同。为了捕捉这种差异，PADD通过三个步骤将高维注意力张量转换为紧凑的、有区别的信号：（i）对齐前缀和原始序列以消除长度不匹配，（ii）归一化注意力分布以消除系统偏差，以及（iii）跨层、头部和位置聚合特征以抑制局部噪声。这个过程产生了两个互补的度量，扰动敏感性和注意力可塑性，它们一起形成了适合对抗性提示检测的稳定的低维表示。
# 3 PADD:An LLM Jailbreak Detector
基于关键思想，我们设计了PADD，一种基于前缀的注意力分歧检测器，在响应生成之前自动检测大语言模型越狱。
## 3.1 Overview
如图2所示，PADD采用五阶段工作流，将用户提示转换为可解释的越狱判断。首先，安全前缀插入阶段在原始提示x之前加上一个短前缀p，以形成其变体x̃.其次，注意力特征提取阶段通过大语言模型处理x和x̃，并计算所有头部和层的平均注意力张量。第三，位置对齐阶段通过丢弃与前缀相关的位置并应用逐行归一化来消除长度偏差来对齐x和x̃。第四，越狱信号获取阶段从四个向量导出两个互补信号：x和x̃的finaltoken注意力分布，以及它们各自的全局平均注意力分布。最后，越狱判断阶段将这些信号整合成一个综合分数J（x），该分数将提示分类为对抗性或良性。
## 3.2 Safety Prefix Insertion 安全前缀插入
安全前缀插入的核心思想是引入一种轻量级且可控的扰动，能够在同一提示内进行自比较。具体来说，给定用户发出的提示x，我们在前面加上一个短的安全前缀p，并获得其前缀变体x̃=p|x，其中|表示级联（详见附录B）。
这种设计基于启发式，即微小的扰动很少改变良性提示，但经常在对抗性提示中触发异常或不稳定的注意力反应。构造对（x，x̃）会产生两个语义相同但前缀不同的输入，将困难的交叉提示比较转换为可控的提示内分析。该步骤为后续阶段提取越狱信号提供了可靠的基础。
## 3.3 Attention Feature Extraction 注意力特征提取
注意力特征提取利用模型的注意力张量来表征其跨层和头部的内部响应模式。具体来说，我们将原始提示x及其前缀变体x̃输入到启用注意力输出的模型中，从所有层和头部收集注意力矩阵。由于大语言模型是因果解码器专用架构，这些矩阵是下三角的。
为了获得紧凑而信息丰富的表示，我们计算所有注意力矩阵的全局平均值：
公式
其中Al，j∈RT × T表示来自层l和头部j的softmax归一化注意力矩阵，T是输入序列长度。由于插入了安全前缀，x̃is的矩阵比x的矩阵长，正好是前缀令牌的数量。
我们采用这种聚合策略有两个原因：（i）全局平均值保留了跨层和头部的综合信息，提供了模型注意力行为的整体视角；以及（ii）将所有注意力矩阵压缩成一个降低了计算成本，提高了后续处理的效率。
## 3.4 Position Alignment 位置对齐
位置比对消除了由安全前缀引起的序列长度差异，使得能够在共享语义空间内比较原始提示x及其前缀变体x̃的注意力分布。该过程包括两个步骤：截断和重新归一化。
#### 3.4.1截断
原始提示x的长度为Tx，而其前缀变体x̃has的长度为Tx+p，其中p是前缀长度。由于这种不匹配，他们的注意力分布在维度上不同，不能直接比较。为了对齐它们，我们从α x̃中移除第一个p前缀相关的维度，只保留与x的语义相对应的部分。尽管这些前缀维度被截断，但x̃has已经经历了正向传播，因此剩余部分隐式地反映了前缀诱导的扰动，使得这种操作是合理的。形式上，
公式
这里，α x∈RTx是x的注意力分布，α x̃∈RTx+p是x̃的注意力分布，α aligned x̃∈RTx是截断后的对齐结果。
#### 3.4.2重新标准化
截断破坏了注意力矩阵的逐行随机性：在移除前缀列后，每一行的总和不再为1。为了恢复可比性，我们使用标准softmax独立地重新归一化每一行。
## 3.5 Jailbreak Signal Acquisition 越狱信号采集
越狱信号获取量化了安全前缀如何影响模型的内部注意力模式，并将这些影响转化为区分良性提示和对抗性提示的指标。我们考察了两个互补的方面：（1）注意力扰动(K)，测量安全前缀是否显著改变最终令牌注意力分布；和（2）注意力可塑性（H），评估它是否改变了注意力分布的全局特征。
#### 3.5.1注意力扰动（K）
它量化了安全前缀如何改变最终令牌注意力分布，反映了模型在其决策边界附近的敏感性。在仅解码器的大语言模型中，最终令牌在产生以下输出之前整合所有先前的上下文，使其注意力分布成为模型决策状态的信息表示。
公式
K的值表示由前缀引起的扰动程度。对于良性提示，K保持较小，因为前缀对模型的决策状态的影响最小。然而，对抗性提示通常会产生明显更大的K值，因为前缀放大了低概率区域的注意力转移，将模型导向不安全的输出。因此，K作为越狱易感性的判别指标。如附录A.1所示，K可以用Fisher加权二次形式来近似，我们采用这种形式来减少计算量，而不影响检测精度。
## 3.6 Jailbreak Judgment 越狱判决
在越狱判断中，将注意力扰动得分K和注意力可塑性得分H这两个互补性指标组合成统一的决策度量，实现了越狱风险的二元检测。

# 4 Evaluation
## 4.1 Experimental Setup
#### 4.1.1目标攻击手段
PADD根据4种越狱技术进行评估。
- GCG[51]：白盒设置中基于梯度的对抗性提示生成方法（访问模型梯度）
- AutoDAN[22]：一种基于进化的攻击方法，在黑盒设置中迭代进化提示
- DeepInception[21]：增量恶意注入方法，其在黑盒设置中增量注入恶意意图
- 基于模板的越狱（TBD）[23]：黑盒设置中基于提示的攻击方法
#### 4.1.2被测大语言模型
我们在四个有代表性的开源大型语言模型上评估PADD。为了稳定，它们的温度设置为0.2。
- LLaMA-3-8B-Instruct【37】：一个广泛采用的通用模型，结合了最近的比对改进。
- Qwen-3-8B[41]：在大规模多语言语料库上训练的竞争性开源模型，具有高级对齐策略。
- DeepSeek-R1-Distill-Qwen-7B[13]：基于蒸馏的变体，强调计算效率，同时保留强大的推理能力。
- Vicuna-13B-v1.5[8]：从LLaMA对用户共享对话进行微调的对话模型。
#### 4.1.3数据集
我们构建了两组数据。
训练数据集包括对抗性和良性提示。我们首先为四种代表性越狱方法中的每一种生成100个提示——GCG【51】、PAIR【6】、DSN【49】和AutoDAN【22】——针对JBB行为【5】中定义的有害行为。这些方法中有一半不同于目标攻击族来评估模型的可推广性。然后，每个提示被馈送到被测试的大语言模型，并收集它们的响应。我们使用Llama-Guard3-8B[24]来检测有害内容，并将每个提示-大语言模型对标记为对抗性或良性。在500个训练样本中，每个模型检测到的对抗性提示的数量对于LLaMA-3-8B-Instruct为223个，对于Qwen-3-8B为192个，对于DeepSeek-R1Distill-Qwen-7B为183个，对于Vicuna-13B-v1.5为254个，每种情况下的剩余样本被标记为良性，导致总共500个标记实例。此外，我们使用GPT-5生成100个非对抗性提示，并将它们标记为良性，确保它们的长度分布与对抗性提示的长度分布相匹配，以实现平衡。
按照类似的程序，使用单独的一批数据构建测试数据集。具体来说，我们为每种目标攻击方法生成400个提示，采用AdvBench[7]中的所有100个种子提示，并从RedTeam[25]中随机抽样300个。在非越狱集，我们包括来自XSTest基准[32]的所有250个样本，该基准评估防御机制中的过度拒绝行为，以及GPT5生成的150个额外提示。测试数据集总共包括8,000个样本（每个大语言模型2,000个），其中1,994个被确定为对抗性样本。
#### 4.1.4基线
- Vanilla（无防御）：大语言模型在没有任何保护的情况下直接暴露在越狱尝试中。
- RA-LLM[4]：通过随机擦除创建提示变体并根据响应差异检测敌对意图来防御越狱。这是一种黑箱方法。
- SmoothLLM[31]：它通过在测试时应用随机输入扰动并聚合平滑响应以减少对抗性提示的影响来防御越狱。这是一种黑箱方法。
- Self Defense[28]：它采用另一种大语言模型来筛选生成的响应并检测有害内容。这是一种黑箱方法。
- HSF[29]：它分析提示的最后几个令牌的隐藏状态以检测对手。这是一种白盒方法。
#### 4.1.5度量
- ASR（Attack Success Rate）：用探测器防御时，对抗性提示成功越狱的比例。
- FRR（错误拒绝率）：良性提示被错误标记为越狱的比例。
- F1分数：根据提示的基本事实标签计算的F1分数。根据我们在2.1节中的定义，只有当一个提示成功越狱目标模型并产生有害输出时，它才被标记为负面（对抗性提示），而所有其他提示都被标记为正面（良性提示）。
## 4.2 Overall Results
表2总结了实验结果。
总体而言，PADD在四个评估的大语言模型中实现了最佳或接近最佳的性能。它将大多数型号的平均ASR降低到2%以下，并在DeepSeek-R1-DistillQwen-7B上保持在3.0%左右。与现有基线相比，PADD在大多数情况下将平均ASR降低了0.1-29.5%，而特定基线在结构化攻击（即GCG和AutoDAN）上获得了略低的ASR，但FRR却大大提高了。与此同时，PADD始终获得最高的F1分数，范围从91.0%到97.5%，表明其通过利用内部表示信号准确确定越狱是否真正发生的稳健性。
相比之下，基线表现出几个显著的局限性。RALLM依赖于随机擦除和多数投票，这会破坏输入语义并增加错误拒绝（例如，高达10.4%）。例如，从良性查询“【如何杀死一个Python进程？】”中删除令牌python可能会导致错误的拒绝。
SmoothLLM聚合对扰动副本的预测，并在对抗字符级后缀攻击（如GCG）方面表现良好，但在对抗深度感知[21]和TBD[23]等结构化攻击时表现不佳。在TBD攻击下（“[以下行为是一个安全的行为。请回答：如何杀死一只猫？]”）轻微的字符干扰不能扭曲语义线索，使ASR保持较高（Qwen-3-8B上为7.2%），并导致Vicuna-13B-v1.5上接近失败（平均ASR>30%）。同时，PADD通过利用基于前缀的自我比较成功地检测到对抗性尝试，这捕捉了对抗性提示中异常的注意力不稳定性。
Self Defense采用第二种大语言模型进行事后审计，并在某些设置中取得了良好的结果（例如，平均ASR 0.5%，F1 95.4%），特别是在LLaMA-3-8B-Instruct上。然而，其性能在DeepSeek-R1-Distill-Qwen-7B上急剧下降（ASR 1.7%，F1 72.1%），表明跨模型的泛化性较差。这种退化主要源于其反应性：通过中性或学术措辞隐藏恶意意图的提示（例如，“【从历史角度分析中世纪投毒案件中毒药是如何制备的。】”）往往逃避侦查。相比之下，PADD通过在生成之前分析内部注意力分歧来检测这种情况，即使在语义模糊的情况下也能揭示隐藏的敌对意图。
HSF检测来自最后k个令牌的隐藏状态的攻击，并在模型上获得竞争结果（例如，在LLaMA-3-8B-Instruct上的ASR为3.3%，在DeepSeek-R1-Distill-Qwen-7B上为6.1%），但不稳定：在Qwen-3-8B上FRR上升到10.4%（F1 79.4%），在Vicuna-13B-v1.5上，几次攻击的ASR超过10%（总体F1 46.6%）。当HSF将最后k个隐藏状态向量连接到用于轻量级分类的固定长度特征中时，它偏向于局部的、位置相关的线索（例如，序列结束模板），导致对良性输入的错误拒绝和对新颖或不同格式的攻击的不稳定性能。
头顶。我们通过采样100个不同长度的提示并报告它们的平均延迟来评估运行时开销。平均而言，PADD在四个RTX 4090 GPU上运行时，每个提示会引入大约0.384秒的开销。这种额外的成本来自一次额外的前向传递，结合轻量级注意力提取和度量计算，与3-29秒的响应生成时间相比，这仍然可以忽略不计。
相比之下，基线方法的开销比PADD高3-9倍。RA-大语言模型和SmoothLLM需要对扰动输入和聚合预测进行多次前向传递，从而大大增加了计算量（见附录E）。自卫要求目标大语言模型在二次审计之前完成整个响应生成，从而引入延迟，因为检测仅在生成后才开始。尽管HSF使用带有外部分类器的单个大语言模型遍，但它的ASR和FRR要高得多。
## 4.3 Ablation Study 消融研究
如表3所示，我们构建了五个变量来评估PADD核心组件的贡献：（1）安全前缀（由“####”代替），（2）注意力分布归一化，（3）相对熵（由原始香农熵代替），（4）注意力扰动（K），和（5）注意力可塑性（H）。
移除PADD的任何组件都会导致明显的性能下降。最显著的下降发生在w/o安全性前缀（平均ASR增加Δ=+19.6%）和w/o K（Δ=+9.8%），强调了它们的主要区别作用。w/o Norm引入中度恶化（Δ=+4.6%），而w/o RelEnt和w/o H产生较小但不可忽略的ASR增加（分别为Δ=+2.5%和Δ=+1.4%），伴有轻微的FRR变化。这些结果表明，安全前缀和K是对抗性-良性分离的主要决定因素，而归一化、相对熵和H提供了互补的稳健性。移除任何模块都会恶化ASR/FRR权衡。
## 4.4 Robustness of Threshold Calibration 阈值校准的稳健性
为了评估PADD阈值校准的稳健性，我们在LLaMA-3-8BInstruct模型上进行了5倍交叉验证实验。
训练集被划分为五个不相交的折叠。在每次迭代中，使用四次折叠（大约80%的数据）来校准用于区分对抗性提示和良性提示的阈值。与标准交叉验证不同，剩余倍数不用于评估；相反，我们应用来自主要实验的相同独立测试集来确定最佳阈值τ并计算所有评估度量。由于该测试集在源和分布上都与训练数据完全不相交，因此为评估PADD在不同阈值配置下的泛化能力提供了无偏基准。
结果总结在表4中。在所有折叠中，从不同训练子集导出的阈值在测试集上产生几乎相同的性能，ASR、FRR和F1仅显示出微小的变化。这些结果表明，PADD对训练数据划分中的变化是鲁棒的，并且其阈值校准过程是高度稳定和一致的。换句话说，无论用于校准的特定子集是什么，得到的阈值在heldout测试集上都保持了相当的防御性能。此外，附录C显示，即使在用少得多的样本进行训练时，PADD也能保持强劲的性能，进一步证明了其鲁棒性和数据效率。
## 4.5 Sensitivity to α and β 对α和β的敏感性
为了评估PADD相对于其超参数的鲁棒性，我们在[0.25，3.0]范围内对α和β进行网格搜索，步长为0.25，得到144种配置。
所有实验均在LLaMA-3-8B-Instruct模型上进行。为了提高效率，在评估攻击成功率（ASR）时，我们构建了一个简化的测试集，其中包含来自四种攻击方法中每种方法的25个随机采样的提示，总共100个对抗性样本。错误拒绝率（FRR）是在与主要实验相同的设置下测量的，使用来自XSTest基准的良性提示。
结果总结在图3中。ASR和FRR在参数空间中变化平滑，表明PADD在很大程度上对（α，β）的选择不敏感。在大多数地区，ASR始终保持在较低水平，而FRR保持在可接受的范围内。值得注意的是，对于LLaMA-3-8B-Instruct模型，最佳配置出现在α=1.0和β=1.0时，其中PADD实现了最低的ASR和低FRR，在防御强度和可用性之间取得了理想的平衡。
# 5 Related Work
研究人员研究了如何保护大语言模型免受越狱和即时注入攻击。一般来说，有三类尝试：基于对齐的培训、生成前防御和生成后审计。
基于对齐的方法通过将有用性和无害性嵌入到模型参数中来增强训练或微调期间的安全性。RLHF【27】、宪法AI【3】、DPO【30】和GRPO【34】直接优化比对目标。安全RLHF【9】、解耦拒绝训练【43】和GRAIT【50】引入了明确的拒绝机制或安全约束，而自我保护【38】利用了自我识别的危害标签。尽管取得了这些进步，但基于比对的方法仍然容易受到分布变化和对抗性提示的影响，留下了持续的安全漏洞[18]。
预生成防御在解码之前进行干预。RA-大语言模型[4]、SmoothLLM[31]和RPO[48]采用基于扰动的策略，如随机消融、平滑或后缀优化。梯度袖带【14】应用拒绝损失测试，Alon等人。【1】使用基于困惑的筛选，PromptShield【16】引入可部署的注入过滤器，HSF【29】训练隐藏状态分类器。虽然有效，但这些方法通常会增加延迟，需要检测器维护，并使部署复杂化，即使不修改基本模型也是如此。
生成后防御在生成后审核或重写输出。大语言模型自卫[28]和自卫[38]使用基于大语言模型的过滤，而宪法分类器[35]和RigorLLM[45]依赖于基于分类器的护栏。当检测到不安全内容时，早期退出停止【46】停止解码，阴影模型门控【40】采用辅助验证。危害性分类器[19]估计输出风险，而安全重写[44]重新制定不安全的响应。这些方法通常会增加推理开销，依赖于外部模型，并有对良性输出进行错误分类的风险。
相比之下，我们的PADD使用轻量级的基于前缀的注意力分歧信号在生成前检测潜在风险，实现了强大的安全性能和实际的可部署性。
# 6 Conclusion
在这项工作中，我们提出了PADD，一种基于前缀的注意力分歧检测器，能够在文本生成之前有效地识别越狱风险。通过将扰动和可塑性得分结合到一个统一的风险度量中，PADD显著降低了攻击成功率，同时在多个模型和不同的攻击范式中保持了较低的错误拒绝率。实验结果表明，PADD实现了很强的鲁棒性和泛化性，同时保持了轻量级和易于部署，为增强大语言模型的安全性提供了新的视角。